# 0-a. Create ssh key to connect to github
ssh-keygen -t rsa

# 0-b. Declare author's commit information
git config --global user.email "hoagkub1999@gmail.com"
git config --global user.name "Hoang Huy Dang"

# 0-c. Set the AWS credentials
aws configure
aws configure set aws_session-token <YOUR AWS SESSION TOKEN>

# 0-d. Create kubernetes cluster on EKS
eksctl create cluster --name hoagkub-cluster --region us-east-1 --nodegroup-name my-nodes --node-type t3.small --nodes 1 --nodes-min 1 --nodes-max 2
 
# 0-e. Switch local kubernetes context to the above created cluster
aws eks --region us-east-1 update-kubeconfig --name hoagkub-cluster
kubectl config current-context # You need to verify the output, check if we're connected the above created cluster
kubectl config view

# 0-f. Do it after you leave your computer for a long time, delete that cluster
eksctl delete cluster --name hoagkub-cluster --region us-east-1

# 1. CREATE YAML CONFIGURATIONS

kubectl get namespace # To ensure you're connected to k8s cluster

# 1.1. CREATE PERSISTENTVOLUMECLAIM

cat > pvc.yaml <<EOF
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgresql-pvc
spec:
  accessModes:
    - ReadWriteOnce
  volumeName: my-manual-pv
  resources:
    requests:
      storage: 1Gi
EOF

# 1.2. CREATE PERSISTENTVOLUME
# accessModes should match above pvc.yaml

cat > pv.yaml <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-manual-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: gp2
  hostPath:
    path: "/mnt/data"
EOF

kubectl get storageclass # Ensure storageClassName same as output of command below

# 1.3. CREATE POSTGRES DEPLOYMENT

# Note the database, user, and passwords in the YAML above. You can change them per your choice. Here are the sample values.
# Database name: hoagkub_psql
# User name: hoagkub_u
# Password: hcupgoan

cat > postgresql-deployment.yaml <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgresql
spec:
  selector:
    matchLabels:
      app: postgresql
  template:
    metadata:
      labels:
        app: postgresql
    spec:
      containers:
      - name: postgresql
        image: postgres:latest
        env:
        - name: POSTGRES_DB
          value: hoagkub_psql
        - name: POSTGRES_USER
          value: hoagkub_u
        - name: POSTGRES_PASSWORD
          value: hcupgoan
        ports:
        - containerPort: 5432
        volumeMounts:
        - mountPath: /var/lib/postgresql/data
          name: postgresql-storage
      volumes:
      - name: postgresql-storage
        persistentVolumeClaim:
          claimName: postgresql-pvc
EOF

# 2. Apply YAML configurations

kubectl apply -f pvc.yaml
kubectl apply -f pv.yaml
kubectl apply -f postgresql-deployment.yaml

# 3. Test Database Connection
kubectl get pods

kubectl exec -it postgresql-688c5c767c-rmdgg -- bash

psql -U hoagkub_u -d hoagkub_psql

\l

\c hoagkub_psql

\dt

# 4. Connecting via Port Forwarding

cat > postgresql-service.yaml <<EOF
apiVersion: v1
kind: Service
metadata:
  name: postgresql-service
spec:
  ports:
  - port: 5432
    targetPort: 5432
  selector:
    app: postgresql
EOF

kubectl apply -f postgresql-service.yaml

# List the services
kubectl get svc

# Set up port-forwarding to `postgresql-service`
kubectl port-forward service/postgresql-service 5433:5432 &
# To view all opened ports, you may click on the PORTS tab on the workspace editor.

# 5. Run Seed Files

apt update
apt install postgresql postgresql-contrib

export DB_PASSWORD=hcupgoan

# PGPASSWORD="$DB_PASSWORD" psql --host 127.0.0.1 -U hoagkub_u -d hoagkub_psql -p 5433 < <FILE_NAME.sql>
PGPASSWORD="$DB_PASSWORD" psql --host 127.0.0.1 -U hoagkub_u -d hoagkub_psql -p 5433 < db/1_create_tables.sql
PGPASSWORD="$DB_PASSWORD" psql --host 127.0.0.1 -U hoagkub_u -d hoagkub_psql -p 5433 < db/2_seed_users.sql
PGPASSWORD="$DB_PASSWORD" psql --host 127.0.0.1 -U hoagkub_u -d hoagkub_psql -p 5433 < db/3_seed_tokens.sql

# 6. Checking the tables
# Checking table
PGPASSWORD="$DB_PASSWORD" psql --host 127.0.0.1 -U hoagkub_u -d hoagkub_psql -p 5433
select *from users;
select* from tokens;
# To exit: \q

# 7. Closing the forwarded ports
# Close port
ps aux | grep 'kubectl port-forward' | grep -v grep | awk '{print $2}' | xargs -r kill

# HOW TO RESUME

# 1. Set Up SSH Key
ssh-keygen -t rsa

# 2. Set Up kubectl Port-forwarding
# Set the AWS credentials
aws configure

# Update the context in your local Kubeconfig file. Replace hoagkub-cluster
# with your kubernetes cluster name in Amazon EKS.
aws eks --region us-east-1 update-kubeconfig --name hoagkub-cluster

kubectl config current-context

# Find the service name of postgresql by listing your services
kubectl get svc

# Set up port-forwarding for the postgresql service
kubectl port-forward svc/postgresql-service 5433:5432 &

# Build the Analytics Application Locally

# 1. Install Dependencies

# Update the local package index with the latest packages from the repositories
apt update

# Install a couple of packages to successfully install postgresql server locally
apt install build-essential libpq-dev

# Update python modules to successfully build the required modules
pip install --upgrade pip setuptools wheel

pip install -r analytics/requirements.txt

# 2. Run the Application

# Set up port forwarding
# kubectl port-forward --namespace default svc/<SERVICE_NAME>-postgresql 5433:5432 &
kubectl port-forward svc/postgresql-service 5433:5432 &
# kubectl get secret --namespace default svc/postgresql-service -o jsonpath="{.data.postgres-password}" | base64 -d
# export POSTGRES_PASSWORD=$(kubectl get svc postgresql-service -o jsonpath="{.data.postgres-password}" | base64 -d)
# Export the password. Replace 
# export POSTGRES_PASSWORD=$(kubectl get secret --namespace default <SERVICE_NAME>-postgresql -o jsonpath="{.data.postgres-password}" | base64 -d)
export POSTGRES_PASSWORD=hcupgoan

export DB_USERNAME=hoagkub_u
export DB_PASSWORD=${POSTGRES_PASSWORD}
export DB_HOST=127.0.0.1
export DB_PORT=5433
export DB_NAME=hoagkub_psql

python analytics/app.py

# 3. Verify the Application
curl 127.0.0.1:5153/api/reports/daily_usage
curl 127.0.0.1:5153/api/reports/user_visits

# Deploy the Analytics Application
# 1. Dockerize the Application
# 1.1. Update Docker Version

apt update
apt install docker-ce docker-ce-cli containerd.io

# 1.2. Build and Run a Docker Image
cd analytics
docker build -t test-coworking-analytics .

docker images

# 1.3. Verify the Docker Image
docker run --network="host" test-coworking-analytics

curl 127.0.0.1:5153/api/reports/daily_usage
curl 127.0.0.1:5153/api/reports/user_visits

# 2. Set up Continuous Integration with CodeBuild

# 2.1. Setting Up

# 3. Deploy the Application

# 3.1. ConfigMap

kubectl get svc

kubectl apply -f deployment/configmap.yaml
kubectl apply -f deployment/coworking.yaml

kubectl get svc

curl a7f36b6661dff4c30ac27662e87634b2-1870009062.us-east-1.elb.amazonaws.com:5153/health_check
curl a7f36b6661dff4c30ac27662e87634b2-1870009062.us-east-1.elb.amazonaws.com:5153/api/reports/daily_usage
curl a7f36b6661dff4c30ac27662e87634b2-1870009062.us-east-1.elb.amazonaws.com:5153/api/reports/user_visits

# CloudWatch

aws iam attach-role-policy \
--role-name eksctl-hoagkub-cluster-nodegroup-m-NodeInstanceRole-WFVI6HLo3bVz \
--policy-arn arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy

aws eks create-addon --addon-name amazon-cloudwatch-observability --cluster-name hoagkub-cluster

kubectl get pods
kubectl get svc

curl a7f36b6661dff4c30ac27662e87634b2-1870009062.us-east-1.elb.amazonaws.com:5153/health_check
curl a7f36b6661dff4c30ac27662e87634b2-1870009062.us-east-1.elb.amazonaws.com:5153/api/reports/daily_usage
curl a7f36b6661dff4c30ac27662e87634b2-1870009062.us-east-1.elb.amazonaws.com:5153/api/reports/user_visits
